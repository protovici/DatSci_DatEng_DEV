{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Library/Packages:\n",
    "from types import ModuleType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import types\n",
    "import pkg_resources\n",
    "import gc\n",
    "import tqdm as tqdm\n",
    "\n",
    "# Visual Library/Packages:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "\n",
    "# Sklearn Library/Packages:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLTK Library/Packages:\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Transformers Library/Packages:\n",
    "import transformers\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# Torch Library/Packages:\n",
    "import torch\n",
    "from torch import nn, optim, argmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Lightning Library/Packages:\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Setting of seeds for comparable results for learning models:\n",
    "SEED_VAL=42\n",
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "torch.manual_seed(SEED_VAL)\n",
    "torch.cuda.manual_seed(SEED_VAL)\n",
    "\n",
    "# Set random seed and random seed for all lightning models:\n",
    "RANDOM_SEED=42\n",
    "pl.seed_everything(RANDOM_SEED)\n",
    "\n",
    "# Processing Device(s):\n",
    "processing_device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_aval = torch.cuda.is_available()\n",
    "\n",
    "def get_imports():\n",
    "\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):            \n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "# Set data paths:\n",
    "IN=r'D:\\download_data\\in'\n",
    "OUT=r'D:\\download_data\\out'\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"###               VERSION TYPES                  ###\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Versions: \", requirements)\n",
    "print(f\"Device(s) to Utilize: \", processing_device)\n",
    "print(f\"Is Torch Availabke?: \", torch_aval)\n",
    "print(f\"# of Devices Found: \", torch.__version__)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"If NVIDIA-SMI is not found, then CUDA isn't available on this device:\")\n",
    "!nvidia-smi\n",
    "gc.collect()\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Clear Torch Cuda Memory: \", torch.cuda.empty_cache())\n",
    "print(\"-----ENVIRONMENT IS COMPLETE & STAGED CORRECTLY-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_train = pd.read_csv(IN + r\"\\PUBHEALTH\\train.tsv\", sep='\\t')\n",
    "pub_health_test = pd.read_csv(IN + r\"\\PUBHEALTH\\test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_train = pub_health_train[pub_health_train['label'] != 'snopes']\n",
    "pub_health_train = pub_health_train[['main_text','label']]\n",
    "pub_health_train = pub_health_train.dropna(subset=['main_text', 'label'])\n",
    "\n",
    "pub_health_test = pub_health_test[pub_health_test['label'] != 'snopes']\n",
    "pub_health_test = pub_health_test[['main_text','label']]\n",
    "pub_health_test = pub_health_test.dropna(subset=['main_text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_train.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_test.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_health_train['label'] = pub_health_train['label'].map({\"true\":0, \"false\":1, \"unproven\":2, \"mixture\":3})\n",
    "pub_health_test['label'] = pub_health_test['label'].map({\"true\":0, \"false\":1, \"unproven\":2, \"mixture\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthClaimClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, max_seq_len=512, batch_size=128, learning_rate = 0.001):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.pretrain_model  = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.pretrain_model.eval()\n",
    "        for param in self.pretrain_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512,4),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "      tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "      tokens_train = tokenizer.batch_encode_plus(\n",
    "          pub_health_train[\"main_text\"].tolist(),\n",
    "          max_length = self.max_seq_len,\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          return_token_type_ids=False\n",
    "      )\n",
    "\n",
    "      tokens_test = tokenizer.batch_encode_plus(\n",
    "          pub_health_test[\"main_text\"].tolist(),\n",
    "          max_length = self.max_seq_len,\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          return_token_type_ids=False\n",
    "      )\n",
    "\n",
    "      self.train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "      self.train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "      self.train_y = torch.tensor(pub_health_train[\"label\"].tolist())\n",
    "\n",
    "      self.test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "      self.test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "      self.test_y = torch.tensor(pub_health_test[\"label\"].tolist())\n",
    "\n",
    "    def forward(self, encode_id, mask):\n",
    "        _, output= self.pretrain_model(encode_id, attention_mask=mask)\n",
    "        output = self.new_layers(output)\n",
    "        return output\n",
    "\n",
    "    def train_dataloader(self):\n",
    "      train_dataset = TensorDataset(self.train_seq, self.train_mask, self.train_y)\n",
    "      self.train_dataloader_obj = DataLoader(train_dataset, batch_size=self.batch_size)\n",
    "      shuffle=True\n",
    "      num_workers=8\n",
    "      return self.train_dataloader_obj\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "      test_dataset = TensorDataset(self.test_seq, self.test_mask, self.test_y)\n",
    "      self.test_dataloader_obj = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "      shuffle=True\n",
    "      num_workers=8\n",
    "      return self.test_dataloader_obj\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "      encode_id, mask, targets = batch\n",
    "      outputs = self(encode_id, mask) \n",
    "      preds = torch.argmax(outputs, dim=1)\n",
    "      train_accuracy = accuracy(preds, targets, task=\"multiclass\", num_classes=4)\n",
    "      loss = self.loss(outputs, targets)\n",
    "      self.log('train_accuracy', train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "      self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "      return {\"loss\":loss, 'train_accuracy': train_accuracy}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "      encode_id, mask, targets = batch\n",
    "      outputs = self.forward(encode_id, mask)\n",
    "      preds = torch.argmax(outputs, dim=1)\n",
    "      test_accuracy = accuracy(preds, targets, task=\"multiclass\", num_classes=4)\n",
    "      loss = self.loss(outputs, targets)\n",
    "      return {\"test_loss\":loss, \"test_accuracy\":test_accuracy}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "      test_outs = []\n",
    "      for test_out in outputs:\n",
    "          out = test_out['test_accuracy']\n",
    "          test_outs.append(out)\n",
    "      total_test_accuracy = torch.stack(test_outs).mean()\n",
    "      self.log('total_test_accuracy', total_test_accuracy, on_step=False, on_epoch=True)\n",
    "      return total_test_accuracy\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "      params = self.parameters()\n",
    "      optimizer = optim.Adam(params=params, lr = self.learning_rate)\n",
    "      return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HealthClaimClassifier()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs = 10, accelerator = 'gpu', devices = -1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e940d8f202f43903b60fbe1bc81a90ad0f1fd887d2741c5640d8a1ebfb812f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
